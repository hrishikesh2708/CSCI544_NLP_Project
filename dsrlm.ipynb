{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "if not os.path.isdir('gpt-baselines'):\n",
    "  # https://drive.google.com/file/d/1diss5doSxvIRixMm3q38a9--8-ncrkhN/view?usp=drive_link -> zip\n",
    "  file_id = \"1diss5doSxvIRixMm3q38a9--8-ncrkhN\"\n",
    "\n",
    "  from pydrive.auth import GoogleAuth\n",
    "  from pydrive.drive import GoogleDrive\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "  from googleapiclient.discovery import build\n",
    "  drive_service = build('drive', 'v3')\n",
    "\n",
    "  import io\n",
    "  from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "  request = drive_service.files().get_media(fileId=file_id)\n",
    "  downloaded = io.BytesIO()\n",
    "  downloader = MediaIoBaseDownload(downloaded, request)\n",
    "  done = False\n",
    "  while done is False:\n",
    "    _, done = downloader.next_chunk()\n",
    "\n",
    "  fileId = drive.CreateFile({'id': file_id })\n",
    "  print(fileId['title'])\n",
    "  fileId.GetContentFile(fileId['title'])\n",
    "  !unzip {fileId['title']}\n",
    "\n",
    "else:\n",
    "  print('Colab contents already exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import scallopy\n",
    "except:\n",
    "  ! pip install clutrr/scallopy-0.1.9-cp310-cp310-manylinux_2_31_x86_64.whl\n",
    "  import scallopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer, XLNetTokenizer, XLNetModel, AutoTokenizer, ElectraModel, T5Model, AlbertModel\n",
    "import numpy as np\n",
    "TRAIN_SPLIT = 32\n",
    "TEST_SPLIT = 8\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from datasets import load_dataset\n",
    "except:\n",
    "  !pip install datasets\n",
    "  from datasets import load_dataset\n",
    "\n",
    "train = load_dataset(\"CLUTRR/v1\", name= \"gen_train234_test2to10\", split=f\"train[:{TRAIN_SPLIT}]\")\n",
    "test = load_dataset(\"CLUTRR/v1\", name= \"gen_train234_test2to10\", split=f\"test[:{TEST_SPLIT}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_id_map = {\n",
    "  'daughter': 0,\n",
    "  'sister': 1,\n",
    "  'son': 2,\n",
    "  'aunt': 3,\n",
    "  'father': 4,\n",
    "  'husband': 5,\n",
    "  'granddaughter': 6,\n",
    "  'brother': 7,\n",
    "  'nephew': 8,\n",
    "  'mother': 9,\n",
    "  'uncle': 10,\n",
    "  'grandfather': 11,\n",
    "  'wife': 12,\n",
    "  'grandmother': 13,\n",
    "  'niece': 14,\n",
    "  'grandson': 15,\n",
    "  'son-in-law': 16,\n",
    "  'father-in-law': 17,\n",
    "  'daughter-in-law': 18,\n",
    "  'mother-in-law': 19,\n",
    "}\n",
    "\n",
    "class CLUTRRDataset:\n",
    "  def __init__(self, root, dataset, split):\n",
    "      self.data = dataset\n",
    "      print('self.data ->',self.data)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    # Context is a list of sentences\n",
    "    context = [s.strip().lower() for s in self.data[i]['story'].split(\".\") if s.strip() != \"\"]\n",
    "\n",
    "    # Query is of type (sub, obj)\n",
    "    query_sub_obj = eval(self.data[i]['query'])\n",
    "    query = (query_sub_obj[0].lower(), query_sub_obj[1].lower())\n",
    "\n",
    "    # Answer is one of 20 classes such as daughter, mother, ...\n",
    "    answer = self.data[i]['target_text']\n",
    "    return ((context, query), answer)\n",
    "\n",
    "  @staticmethod\n",
    "  def collate_fn(batch):\n",
    "    queries = [query for ((_, query), _) in batch]\n",
    "    contexts = [fact for ((context, _), _) in batch for fact in context]\n",
    "    context_lens = [len(context) for ((context, _), _) in batch]\n",
    "    context_splits = [(sum(context_lens[:i]), sum(context_lens[:i + 1])) for i in range(len(context_lens))]\n",
    "    answers = torch.stack([torch.tensor(relation_id_map[answer]) for (_, answer) in batch])\n",
    "    return ((contexts, queries, context_splits), answers)\n",
    "\n",
    "\n",
    "def clutrr_loader(root, train , test, batch_size):\n",
    "  train_dataset = CLUTRRDataset(root, train, \"train\")\n",
    "  train_loader = DataLoader(train_dataset, batch_size, collate_fn=CLUTRRDataset.collate_fn, shuffle=True)\n",
    "  test_dataset = CLUTRRDataset(root, test, \"test\")\n",
    "  test_loader = DataLoader(test_dataset, batch_size, collate_fn=CLUTRRDataset.collate_fn, shuffle=True)\n",
    "  return (train_loader, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CLUTRRModel(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    scl_file_name,\n",
    "    tokenizer,\n",
    "    pretrained_model_name,\n",
    "    pretrained_model_path,\n",
    "    device=\"cpu\",\n",
    "    num_mlp_layers=1,\n",
    "    debug=False,\n",
    "    no_fine_tune_roberta=False,\n",
    "    use_softmax=False,\n",
    "    provenance=\"difftopbottomkclauses\",\n",
    "    train_top_k=3,\n",
    "    test_top_k=3,\n",
    "    scallop_softmax=False,\n",
    "  ):\n",
    "    super(CLUTRRModel, self).__init__()\n",
    "\n",
    "    # Options\n",
    "    self.device = device\n",
    "    self.debug = debug\n",
    "    self.no_fine_tune_roberta = no_fine_tune_roberta\n",
    "    self.scallop_softmax = scallop_softmax\n",
    "\n",
    "    # Roberta as embedding extraction model\n",
    "\n",
    "\n",
    "    self.tokenizer = tokenizer.from_pretrained(pretrained_model_path, local_files_only=False, add_prefix_space=True)\n",
    "    self.model = pretrained_model_name.from_pretrained(pretrained_model_path)\n",
    "    self.embed_dim = self.model.config.hidden_size\n",
    "\n",
    "    # Entity embedding\n",
    "    self.relation_extraction = MLP(self.embed_dim * 3, self.embed_dim, len(relation_id_map), num_layers=num_mlp_layers, sigmoid=not use_softmax, softmax=use_softmax)\n",
    "\n",
    "    # Scallop reasoning context\n",
    "    self.scallop_ctx = scallopy.ScallopContext(provenance=provenance, train_k=train_top_k, test_k=test_top_k)\n",
    "    self.scallop_ctx.import_file(\"./clutrr/scl/clutrr_constraints.scl\")\n",
    "    self.scallop_ctx.set_non_probabilistic([\"question\"])\n",
    "\n",
    "    if self.debug:\n",
    "      self.reason = self.scallop_ctx.forward_function(output_mappings={\"answer\": list(range(len(relation_id_map))), \"violate_ic\": [False, True]}, dispatch=\"single\", debug_provenance=True, retain_graph=True)\n",
    "    else:\n",
    "      self.reason = self.scallop_ctx.forward_function(output_mappings={\"answer\": list(range(len(relation_id_map))), \"violate_ic\": [False, True]}, retain_graph=True)\n",
    "\n",
    "\n",
    "  def _preprocess_contexts(self, contexts, context_splits):\n",
    "    clean_context_splits = []\n",
    "    clean_contexts = []\n",
    "    name_token_indices_maps = []\n",
    "    for (_, (start, end)) in enumerate(context_splits):\n",
    "      skip_next = False\n",
    "      skip_until = 0\n",
    "      curr_clean_contexts = []\n",
    "      curr_name_token_indices_maps = []\n",
    "      for (j, sentence) in zip(range(start, end), contexts[start:end]):\n",
    "        # It is possible to skip a sentence because the previous one includes the current one.\n",
    "        if skip_next:\n",
    "          if j >= skip_until:\n",
    "            skip_next = False\n",
    "          continue\n",
    "\n",
    "        # Get all the names of the current sentence\n",
    "        names = re.findall(\"\\\\[(\\w+)\\\\]\", sentence)\n",
    "\n",
    "        # Check if we need to include the next sentence(s) as well\n",
    "        num_sentences = 1\n",
    "        union_sentence = f\"{sentence}\"\n",
    "        for k in range(j + 1, end):\n",
    "          next_sentence = contexts[k]\n",
    "          next_sentence_names = re.findall(\"\\\\[(\\w+)\\\\]\", next_sentence)\n",
    "          if len(names) == 1 or len(next_sentence_names) == 1:\n",
    "            if len(next_sentence_names) > 0:\n",
    "              num_sentences += 1\n",
    "              union_sentence += f\". {next_sentence}\"\n",
    "              names += next_sentence_names\n",
    "            skip_next = True\n",
    "            if len(next_sentence_names) == 1:\n",
    "              skip_until = k - 1\n",
    "            else:\n",
    "              skip_until = k\n",
    "          else:\n",
    "            break\n",
    "\n",
    "        # Deduplicate the names\n",
    "        names = set(names)\n",
    "\n",
    "        # Debug number of sentences\n",
    "        if self.debug and num_sentences > 1:\n",
    "          print(f\"number of sentences: {num_sentences}, number of names: {len(names)}; {names}\")\n",
    "          print(\"Sentence:\", union_sentence)\n",
    "\n",
    "        # Then split the context by `[` and `]` so that names are isolated in its own string\n",
    "        splitted = [u.strip() for t in union_sentence.split(\"[\") for u in t.split(\"]\") if u.strip() != \"\"]\n",
    "\n",
    "        # Get the ids of the name in the `splitted` array\n",
    "        is_name_ids = {s: [j for (j, sp) in enumerate(splitted) if sp == s] for s in names}\n",
    "\n",
    "        # Get the splitted input_ids\n",
    "        splitted_input_ids_raw = self.tokenizer(splitted).input_ids\n",
    "        splitted_input_ids = [ids[:-1] if j == 0 else ids[1:] if j == len(splitted_input_ids_raw) - 1 else ids[1:-1] for (j, ids) in enumerate(splitted_input_ids_raw)]\n",
    "        index_counter = 0\n",
    "        splitted_input_indices = []\n",
    "        for (j, l) in enumerate(splitted_input_ids):\n",
    "          begin_offset = 1 if j == 0 else 0\n",
    "          end_offset = 1 if j == len(splitted_input_ids) - 1 else 0\n",
    "          quote_s_offset = 1 if \"'s\" in splitted[j] and splitted[j].index(\"'s\") == 0 else 0\n",
    "          splitted_input_indices.append(list(range(index_counter + begin_offset, index_counter + len(l) - end_offset - quote_s_offset)))\n",
    "          index_counter += len(l) - quote_s_offset\n",
    "\n",
    "        # Get the token indices for each name\n",
    "        name_token_indices = {s: [k for phrase_id in is_name_ids[s] for k in splitted_input_indices[phrase_id]] for s in names}\n",
    "\n",
    "        # Clean up the sentence and add it to the batch\n",
    "        clean_sentence = union_sentence.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "        # Preprocess the context\n",
    "        curr_clean_contexts.append(clean_sentence)\n",
    "        curr_name_token_indices_maps.append(name_token_indices)\n",
    "\n",
    "      # Add this batch into the overall list; record the splits\n",
    "      curr_size = len(curr_clean_contexts)\n",
    "      clean_context_splits.append((0, curr_size) if len(clean_context_splits) == 0 else (clean_context_splits[-1][1], clean_context_splits[-1][1] + curr_size))\n",
    "      clean_contexts += curr_clean_contexts\n",
    "      name_token_indices_maps += curr_name_token_indices_maps\n",
    "\n",
    "    # Return the preprocessed contexts and splits\n",
    "    return (clean_contexts, clean_context_splits, name_token_indices_maps)\n",
    "\n",
    "  def _extract_relations(self, clean_contexts, clean_context_splits, name_token_indices_maps):\n",
    "    # encode the contexts into overall tensors\n",
    "    context_tokenized_result = self.tokenizer(clean_contexts, padding=True, return_tensors=\"pt\")\n",
    "    context_input_ids = context_tokenized_result.input_ids.to(self.device)\n",
    "    context_attention_mask = context_tokenized_result.attention_mask.to(self.device)\n",
    "    encoded_contexts = self.model(context_input_ids, context_attention_mask)\n",
    "    # encoded_contexts = self.model(**context_tokenized_result)\n",
    "    if self.no_fine_tune_roberta:\n",
    "      roberta_embedding = encoded_contexts.last_hidden_state.detach()\n",
    "    else:\n",
    "      roberta_embedding = encoded_contexts.last_hidden_state\n",
    "\n",
    "    # Extract features corresponding to the names for each context\n",
    "    splits, name_pairs, name_pairs_features = [], [], []\n",
    "\n",
    "    for (begin, end) in clean_context_splits:\n",
    "      curr_datapoint_name_pairs = []\n",
    "      curr_datapoint_name_pairs_features = []\n",
    "      curr_sentence_rep = []\n",
    "\n",
    "      for (j, name_token_indices) in zip(range(begin, end), name_token_indices_maps[begin:end]):\n",
    "        # Generate the feature_maps\n",
    "        feature_maps = {}\n",
    "        curr_sentence_rep.append(torch.mean(roberta_embedding[j, :sum(context_attention_mask[j]), :], dim=0))\n",
    "        for (name, token_indices) in name_token_indices.items():\n",
    "          token_features = roberta_embedding[j, token_indices, :]\n",
    "\n",
    "          # Use max pooling to join the features\n",
    "          agg_token_feature = torch.max(token_features, dim=0).values\n",
    "          feature_maps[name] = agg_token_feature\n",
    "\n",
    "        # Generate name pairs\n",
    "        names = list(name_token_indices.keys())\n",
    "        curr_sentence_name_pairs = [(m, n) for m in names for n in names if m != n]\n",
    "        curr_datapoint_name_pairs += curr_sentence_name_pairs\n",
    "        curr_datapoint_name_pairs_features += [torch.cat((feature_maps[x], feature_maps[y])) for (x, y) in curr_sentence_name_pairs]\n",
    "\n",
    "      global_rep = torch.mean(torch.stack(curr_sentence_rep), dim=0)\n",
    "\n",
    "      # Generate the pairs for this datapoint\n",
    "      num_name_pairs = len(curr_datapoint_name_pairs)\n",
    "      splits.append((0, num_name_pairs) if len(splits) == 0 else (splits[-1][1], splits[-1][1] + num_name_pairs))\n",
    "      name_pairs += curr_datapoint_name_pairs\n",
    "      name_pairs_features += curr_datapoint_name_pairs_features\n",
    "\n",
    "    # Stack all the features into the same big tensor\n",
    "    name_pairs_features = torch.cat((torch.stack(name_pairs_features), global_rep.repeat(len(name_pairs_features), 1)), dim=1)\n",
    "\n",
    "    # Use MLP to extract relations between names\n",
    "    name_pair_relations = self.relation_extraction(name_pairs_features)\n",
    "\n",
    "    # Return the extracted relations and their corresponding symbols\n",
    "    return (splits, name_pairs, name_pair_relations)\n",
    "\n",
    "  def _extract_facts(self, splits, name_pairs, name_pair_relations, queries):\n",
    "    context_facts, context_disjunctions, question_facts = [], [], []\n",
    "    num_pairs_processed = 0\n",
    "\n",
    "    # Generate facts for each context\n",
    "    for (i, (begin, end)) in enumerate(splits):\n",
    "      # First combine the name_pair features if there are multiple of them, using max pooling\n",
    "      name_pair_to_relations_map = {}\n",
    "      for (j, name_pair) in zip(range(begin, end), name_pairs[begin:end]):\n",
    "        name_pair_to_relations_map.setdefault(name_pair, []).append(name_pair_relations[j])\n",
    "      name_pair_to_relations_map = {k: torch.max(torch.stack(v), dim=0).values for (k, v) in name_pair_to_relations_map.items()}\n",
    "\n",
    "      # Generate facts and disjunctions\n",
    "      curr_context_facts = []\n",
    "      curr_context_disjunctions = []\n",
    "      for ((sub, obj), relations) in name_pair_to_relations_map.items():\n",
    "        curr_context_facts += [(relations[k], (k, sub, obj)) for k in range(len(relation_id_map))]\n",
    "        curr_context_disjunctions.append(list(range(len(curr_context_facts) - 20, len(curr_context_facts))))\n",
    "      context_facts.append(curr_context_facts)\n",
    "      context_disjunctions.append(curr_context_disjunctions)\n",
    "      question_facts.append([queries[i]])\n",
    "\n",
    "      # Increment the num_pairs processed for the next datapoint\n",
    "      num_pairs_processed += len(name_pair_to_relations_map)\n",
    "\n",
    "    # Return the facts generated\n",
    "    return (context_facts, context_disjunctions, question_facts)\n",
    "\n",
    "  def forward(self, x, phase='train'):\n",
    "    (contexts, queries, context_splits) = x\n",
    "\n",
    "    # Debug prints\n",
    "    if self.debug:\n",
    "      print(contexts)\n",
    "      print(queries)\n",
    "\n",
    "    # Go though the preprocessing, RoBERTa model forwarding, and facts extraction steps\n",
    "    (clean_contexts, clean_context_splits, name_token_indices_maps) = self._preprocess_contexts(contexts, context_splits)\n",
    "    (splits, name_pairs, name_pair_relations) = self._extract_relations(clean_contexts, clean_context_splits, name_token_indices_maps)\n",
    "    (context_facts, context_disjunctions, question_facts) = self._extract_facts(splits, name_pairs, name_pair_relations, queries)\n",
    "\n",
    "    # Run Scallop to reason the result relation\n",
    "    result = self.reason(context=context_facts, question=question_facts, disjunctions={\"context\": context_disjunctions})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, in_dim: int, embed_dim: int, out_dim: int, num_layers: int = 0, softmax = False, normalize = False, sigmoid = False):\n",
    "    super(MLP, self).__init__()\n",
    "    layers = []\n",
    "    layers += [nn.Linear(in_dim, embed_dim), nn.ReLU()]\n",
    "    for _ in range(num_layers):\n",
    "      layers += [nn.Linear(embed_dim, embed_dim), nn.ReLU()]\n",
    "    layers += [nn.Linear(embed_dim, out_dim)]\n",
    "    self.model = nn.Sequential(*layers)\n",
    "    self.softmax = softmax\n",
    "    self.normalize = normalize\n",
    "    self.sigmoid = sigmoid\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    if self.softmax: x = nn.functional.softmax(x, dim=1)\n",
    "    if self.normalize: x = nn.functional.normalize(x)\n",
    "    if self.sigmoid: x = torch.sigmoid(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, train_loader, test_loader, device, model_dir, model_name, learning_rate, results, **args):\n",
    "    self.device = device\n",
    "    self.model = CLUTRRModel(device=device, **args).to(device)\n",
    "    self.model_dir = model_dir\n",
    "    self.model_name = model_name\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    self.train_loader = train_loader\n",
    "    self.test_loader = test_loader\n",
    "    self.min_test_loss = 10000000000.0\n",
    "    self.max_accu = float(\"-inf\")\n",
    "    self.training_accu = 0\n",
    "    self.results = results\n",
    "\n",
    "  def loss(self, y_pred, y):\n",
    "    result = y_pred['answer']\n",
    "    ic = y_pred['violate_ic']\n",
    "    (_, dim) = result.shape\n",
    "    gt = torch.stack([torch.tensor([1.0 if i == t else 0.0 for i in range(dim)]) for t in y]).to(self.device)\n",
    "    result_loss = nn.functional.binary_cross_entropy(result, gt)\n",
    "    ic_loss = 0.1 * nn.functional.nll_loss(ic[:, 1], torch.tensor([0] * ic.shape[0]).to(self.device))\n",
    "    return result_loss + ic_loss\n",
    "\n",
    "  def accuracy(self, y_pred, y):\n",
    "    batch_size = len(y)\n",
    "    result = y_pred['answer'].detach()\n",
    "    pred = torch.argmax(result, dim=1)\n",
    "    num_correct = len([() for i, j in zip(pred, y) if i == j])\n",
    "    return (num_correct, batch_size)\n",
    "\n",
    "  def train(self, num_epochs):\n",
    "    for i in range(1, num_epochs + 1):\n",
    "      train_avg_loss, train_correct_perc = self.train_epoch(i)\n",
    "      test_avg_loss, test_correct_perc = self.test_epoch(i)\n",
    "    return self.results\n",
    "\n",
    "  def train_epoch(self, epoch):\n",
    "    self.model.train()\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    iterator = tqdm(self.train_loader)\n",
    "    for (i, (x, y)) in enumerate(iterator):\n",
    "      self.optimizer.zero_grad()\n",
    "      y_pred = self.model(x, 'train')\n",
    "      loss = self.loss(y_pred, y)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "      (num_correct, batch_size) = self.accuracy(y_pred, y)\n",
    "      total_count += batch_size\n",
    "      total_correct += num_correct\n",
    "      correct_perc = 100. * total_correct / total_count\n",
    "      avg_loss = total_loss / (i + 1)\n",
    "\n",
    "      iterator.set_description(f\"[Train Epoch {epoch}] Avg Loss: {avg_loss}, Accuracy: {total_correct}/{total_count} ({correct_perc:.2f}%)\")\n",
    "      self.training_accu = total_correct/total_count * 100\n",
    "    return avg_loss, correct_perc\n",
    "\n",
    "  def test_epoch(self, epoch):\n",
    "    self.model.eval()\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "      iterator = tqdm(self.test_loader)\n",
    "      for (i, (x, y)) in enumerate(iterator):\n",
    "        y_pred = self.model(x, 'test')\n",
    "        loss = self.loss(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        (num_correct, batch_size) = self.accuracy(y_pred, y)\n",
    "        total_count += batch_size\n",
    "        total_correct += num_correct\n",
    "        correct_perc = 100. * total_correct / total_count\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        iterator.set_description(f\"[Test Epoch {epoch}] Avg Loss: {avg_loss}, Accuracy: {total_correct}/{total_count} ({correct_perc:.2f}%)\")\n",
    "\n",
    "    # Save model\n",
    "    if total_correct / total_count >= self.max_accu:\n",
    "      self.max_accu = total_correct / total_count\n",
    "      torch.save(self.model, os.path.join(self.model_dir, f\"{self.model_name}.best.model\"))\n",
    "      self.results[self.model_name][\"Train\"] = {\"Accuracy\": self.training_accu}\n",
    "      self.results[self.model_name][\"Test\"] = {\"Accuracy\": self.max_accu}\n",
    "    # torch.save(self.model, os.path.join(self.model_dir, f\"{self.model_name}.latest.model\"))\n",
    "\n",
    "    return avg_loss, correct_perc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed_all(1234)\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "transformers.set_seed(1234)\n",
    "model_dir = \"./model/clutrr\"\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "data_root =  \"./data\"\n",
    "(train_loader, test_loader) = clutrr_loader(data_root, train, test, BATCH_SIZE)\n",
    "\n",
    "results = {\n",
    "    \"DSRLM-Roberta\" : {\"Train\":{\"Accuracy\":None}, \"Test\":{\"Accuracy\":None}},\n",
    "    \"DSRLM-Electra\" : {\"Train\":{\"Accuracy\":None}, \"Test\":{\"Accuracy\":None}},\n",
    "    \"DSRLM-XLNet\" : {\"Train\":{\"Accuracy\":None}, \"Test\":{\"Accuracy\":None}},\n",
    "    \"DSRLM-Albert\" : {\"Train\":{\"Accuracy\":None}, \"Test\":{\"Accuracy\":None}},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSRLM-Roberta\n",
    "trainer = Trainer(train_loader, test_loader, \"cpu\", model_dir, \"DSRLM-Roberta\", 0.00001, num_mlp_layers=2, debug=False, provenance=\"difftopkproofs\", train_top_k=3, test_top_k=3, use_softmax=True, no_fine_tune_roberta=False, scallop_softmax = False, scl_file_name=\"clutrr_constraints.scl\", tokenizer=RobertaTokenizer , pretrained_model_name=RobertaModel , pretrained_model_path=\"roberta-base\", results=results)\n",
    "results = trainer.train(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSRLM-Electra\n",
    "trainer = Trainer(train_loader, test_loader, \"cpu\", model_dir, \"DSRLM-Electra\", 0.00001, num_mlp_layers=2, debug=False, provenance=\"difftopkproofs\", train_top_k=3, test_top_k=3, use_softmax=True, no_fine_tune_roberta=False, scallop_softmax = False, scl_file_name=\"clutrr_constraints.scl\", tokenizer=AutoTokenizer, pretrained_model_name=ElectraModel ,pretrained_model_path=\"google/electra-small-discriminator\", results=results)\n",
    "results = trainer.train(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSRLM-XLNet\n",
    "trainer = Trainer(train_loader, test_loader, \"cpu\", model_dir, \"DSRLM-XLNet\", 0.00001, num_mlp_layers=2, debug=False, provenance=\"difftopkproofs\", train_top_k=3, test_top_k=3, use_softmax=True, no_fine_tune_roberta=False, scallop_softmax = False, scl_file_name=\"clutrr_constraints.scl\", tokenizer=XLNetTokenizer, pretrained_model_name=XLNetModel,pretrained_model_path=\"xlnet-base-cased\", results=results)\n",
    "results = trainer.train(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSRLM-Albert\n",
    "trainer = Trainer(train_loader, test_loader, \"cpu\", model_dir, \"DSRLM-Albert\", 0.00001, num_mlp_layers=2, debug=False, provenance=\"difftopkproofs\", train_top_k=3, test_top_k=3, use_softmax=True, no_fine_tune_roberta=False, scallop_softmax = False, scl_file_name=\"clutrr_constraints.scl\", tokenizer=AutoTokenizer, pretrained_model_name=AlbertModel ,pretrained_model_path=\"albert/albert-base-v2\", results=results)\n",
    "results = trainer.train(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
