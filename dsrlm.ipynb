{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scallopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaModel, RobertaTokenizer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscallopy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scallopy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "import scallopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.tokenization_roberta.RobertaTokenizer"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RobertaTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: datasets in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (8.1.2)\n",
      "Requirement already satisfied: torch in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: transformers in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (4.39.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scallopy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scallopy\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install pandas datasets ipywidgets torch transformers scallopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset(\"CLUTRR/v1\", name= \"gen_train234_test2to10\", split=\"train\")\n",
    "test = load_dataset(\"CLUTRR/v1\", name= \"gen_train234_test2to10\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_id_map = {\n",
    "  'daughter': 0,\n",
    "  'sister': 1,\n",
    "  'son': 2,\n",
    "  'aunt': 3,\n",
    "  'father': 4,\n",
    "  'husband': 5,\n",
    "  'granddaughter': 6,\n",
    "  'brother': 7,\n",
    "  'nephew': 8,\n",
    "  'mother': 9,\n",
    "  'uncle': 10,\n",
    "  'grandfather': 11,\n",
    "  'wife': 12,\n",
    "  'grandmother': 13,\n",
    "  'niece': 14,\n",
    "  'grandson': 15,\n",
    "  'son-in-law': 16,\n",
    "  'father-in-law': 17,\n",
    "  'daughter-in-law': 18,\n",
    "  'mother-in-law': 19,\n",
    "}\n",
    "\n",
    "class CLUTRRDataset:\n",
    "  def __init__(self, root, dataset, split):\n",
    "    # self.dataset_dir = os.path.join(root, f\"{dataset}/\")\n",
    "    # self.file_names = [os.path.join(self.dataset_dir, d) for d in os.listdir(self.dataset_dir) if f\"_{split}.csv\" in d]\n",
    "    # self.data = [row for f in self.file_names for row in list(csv.reader(open(f)))[1:]]\n",
    "      self.data = dataset\n",
    "    \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    # Context is a list of sentences\n",
    "    context = [s.strip().lower() for s in self.data[i][2].split(\".\") if s.strip() != \"\"]\n",
    "\n",
    "    # Query is of type (sub, obj)\n",
    "    query_sub_obj = eval(self.data[i][3])\n",
    "    query = (query_sub_obj[0].lower(), query_sub_obj[1].lower())\n",
    "\n",
    "    # Answer is one of 20 classes such as daughter, mother, ...\n",
    "    answer = self.data[i][5]\n",
    "    return ((context, query), answer)\n",
    "\n",
    "  @staticmethod\n",
    "  def collate_fn(batch):\n",
    "    queries = [query for ((_, query), _) in batch]\n",
    "    contexts = [fact for ((context, _), _) in batch for fact in context]\n",
    "    context_lens = [len(context) for ((context, _), _) in batch]\n",
    "    context_splits = [(sum(context_lens[:i]), sum(context_lens[:i + 1])) for i in range(len(context_lens))]\n",
    "    answers = torch.stack([torch.tensor(relation_id_map[answer]) for (_, answer) in batch])\n",
    "    return ((contexts, queries, context_splits), answers)\n",
    "\n",
    "\n",
    "def clutrr_loader(root, train , test, batch_size):\n",
    "  train_dataset = CLUTRRDataset(root, train, \"train\")\n",
    "  train_loader = DataLoader(train_dataset, batch_size, collate_fn=CLUTRRDataset.collate_fn, shuffle=True)\n",
    "  test_dataset = CLUTRRDataset(root, test, \"test\")\n",
    "  test_loader = DataLoader(test_dataset, batch_size, collate_fn=CLUTRRDataset.collate_fn, shuffle=True)\n",
    "  return (train_loader, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CLUTRRModel(nn.Module):\n",
    "  def __init__(self, device=\"cpu\", num_mlp_layers=0, debug=False, use_last_hidden_state=False):\n",
    "    super(CLUTRRModel, self).__init__()\n",
    "\n",
    "    # Options\n",
    "    self.device = device\n",
    "    self.debug = debug\n",
    "    self.use_last_hidden_state = use_last_hidden_state\n",
    "\n",
    "    # Roberta as embedding extraction model\n",
    "    self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", local_files_only=False, add_prefix_space=True)\n",
    "    self.roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "    self.embed_dim = self.roberta_model.config.hidden_size\n",
    "\n",
    "    # Entity embedding\n",
    "    self.relation_extraction = MLP(self.embed_dim, self.embed_dim, len(relation_id_map), num_layers=num_mlp_layers, sigmoid=True)\n",
    "\n",
    "    # Scallop reasoning context\n",
    "    self.scallop_ctx = scallopy.ScallopContext(\"difftopbottomkclauses\", k=3)\n",
    "    self.scallop_ctx.import_file(os.path.abspath(os.path.join(os.path.abspath(__file__), \"../scl/clutrr.scl\")))\n",
    "    self.scallop_ctx.set_non_probabilistic([\"question\"])\n",
    "    if self.debug:\n",
    "      self.reason = self.scallop_ctx.forward_function(\"answer\", list(range(len(relation_id_map))), dispatch=\"single\", debug_provenance=True)\n",
    "    else:\n",
    "      self.reason = self.scallop_ctx.forward_function(\"answer\", list(range(len(relation_id_map))))\n",
    "\n",
    "  def forward(self, x):\n",
    "    (contexts, queries, context_splits) = x\n",
    "    batch_size = len(context_splits)\n",
    "\n",
    "    if self.debug:\n",
    "      print(contexts)\n",
    "      print(queries)\n",
    "\n",
    "    # Preprocess sentences\n",
    "    relation_splits = []\n",
    "    relation_sentences = []\n",
    "    relation_name_pairs = []\n",
    "    for (_, (start, end)) in enumerate(context_splits):\n",
    "      curr_relation_sentences = []\n",
    "      curr_name_pairs = []\n",
    "      skip_next = False\n",
    "      skip_until = 0\n",
    "      for (j, sentence) in zip(range(start, end), contexts[start:end]):\n",
    "        # It is possible to skip a sentence because the previous one includes the current one.\n",
    "        if skip_next:\n",
    "          if j >= skip_until:\n",
    "            skip_next = False\n",
    "          continue\n",
    "\n",
    "        # Get all the names of the current sentence\n",
    "        names = re.findall(\"\\\\[(\\w+)\\\\]\", sentence)\n",
    "\n",
    "        # Check if we need to include the next sentence(s) as well\n",
    "        num_sentences_limit = 4\n",
    "        num_sentences = 1\n",
    "        union_sentence = f\"{sentence}\"\n",
    "        for k in range(j + 1, end):\n",
    "          next_sentence = contexts[k]\n",
    "          next_sentence_names = re.findall(\"\\\\[(\\w+)\\\\]\", next_sentence)\n",
    "          if (len(names) == 1 or len(next_sentence_names) == 1) and num_sentences < num_sentences_limit:\n",
    "            if len(next_sentence_names) > 0:\n",
    "              num_sentences += 1\n",
    "              union_sentence += f\". {next_sentence}\"\n",
    "              names += next_sentence_names\n",
    "            skip_next = True\n",
    "            if len(next_sentence_names) == 1:\n",
    "              skip_until = k - 1\n",
    "            else:\n",
    "              skip_until = k\n",
    "          else:\n",
    "            break\n",
    "\n",
    "        # Deduplicate the names\n",
    "        names = list(dict.fromkeys(names))\n",
    "\n",
    "        # Debug number of sentences\n",
    "        if self.debug and num_sentences > 1:\n",
    "          print(f\"number of sentences: {num_sentences}, number of names: {len(names)}; {names}\")\n",
    "          print(\"Sentence:\", union_sentence)\n",
    "\n",
    "        # Clean up the sentence and add it to the batch\n",
    "        clean_sentence = union_sentence.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        curr_relation_sentences += [f\"{clean_sentence}. the relation between {names[k]} and {names[l]} is?\" for k in range(len(names)) for l in range(len(names)) if k != l]\n",
    "        curr_name_pairs += [(k, l) for k in names for l in names if k != l]\n",
    "\n",
    "      # Construct the current datatpoint\n",
    "      curr_split = (0, len(curr_relation_sentences)) if len(relation_sentences) == 0 else (relation_splits[-1][1], relation_splits[-1][1] + len(curr_relation_sentences))\n",
    "      relation_sentences += curr_relation_sentences\n",
    "      relation_name_pairs += curr_name_pairs\n",
    "      relation_splits.append(curr_split)\n",
    "\n",
    "    # Embed all these sentences\n",
    "    sentence_tokens = self.tokenizer(relation_sentences, padding=True, return_tensors=\"pt\")\n",
    "    sentence_input_ids = sentence_tokens[\"input_ids\"].to(self.device)\n",
    "    sentence_attention_mask = sentence_tokens[\"attention_mask\"].to(self.device)\n",
    "    encoded_sentence = self.roberta_model(sentence_input_ids, sentence_attention_mask)\n",
    "    if self.use_last_hidden_state:\n",
    "      sentence_embeddings = encoded_sentence.last_hidden_state[:, 0, :]\n",
    "    else:\n",
    "      sentence_embeddings = encoded_sentence.pooler_output\n",
    "    relations = self.relation_extraction(sentence_embeddings)\n",
    "\n",
    "    # Construct facts\n",
    "    question_facts = [[] for _ in range(batch_size)]\n",
    "    context_facts = [[] for _ in range(batch_size)]\n",
    "    for (i, (start, end)) in enumerate(relation_splits):\n",
    "      question_facts[i] = [queries[i]]\n",
    "      context_facts[i] = [(relations[j, k], (k, relation_name_pairs[j][0], relation_name_pairs[j][1])) for j in range(start, end) for k in range(len(relation_id_map))]\n",
    "\n",
    "    # Run scallop\n",
    "    result = self.reason(question=question_facts, context=context_facts)\n",
    "\n",
    "    # Softmax the result\n",
    "    result = nn.functional.softmax(result, dim=1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, train_loader, test_loader, device, learning_rate, **args):\n",
    "    self.device = device\n",
    "    self.model = CLUTRRModel(device=device, **args).to(device)\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    self.train_loader = train_loader\n",
    "    self.test_loader = test_loader\n",
    "\n",
    "  def loss(self, y_pred, y):\n",
    "    (_, dim) = y_pred.shape\n",
    "    gt = torch.stack([torch.tensor([1.0 if i == t else 0.0 for i in range(dim)]) for t in y])\n",
    "    return nn.functional.binary_cross_entropy(y_pred, gt)\n",
    "\n",
    "  def accuracy(self, y_pred, y):\n",
    "    batch_size = len(y)\n",
    "    pred = torch.argmax(y_pred, dim=1)\n",
    "    num_correct = len([() for i, j in zip(pred, y) if i == j])\n",
    "    return (num_correct, batch_size)\n",
    "\n",
    "  def train(self, num_epochs):\n",
    "    for i in range(num_epochs):\n",
    "      self.train_epoch(i)\n",
    "      self.test_epoch(i)\n",
    "\n",
    "  def train_epoch(self, epoch):\n",
    "    self.model.train()\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    iterator = tqdm(self.train_loader)\n",
    "    for (i, (x, y)) in enumerate(iterator):\n",
    "      self.optimizer.zero_grad()\n",
    "      y_pred = self.model(x).to(\"cpu\")\n",
    "      loss = self.loss(y_pred, y)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "      (num_correct, batch_size) = self.accuracy(y_pred, y)\n",
    "      total_count += batch_size\n",
    "      total_correct += num_correct\n",
    "      correct_perc = 100. * total_correct / total_count\n",
    "      avg_loss = total_loss / (i + 1)\n",
    "\n",
    "      iterator.set_description(f\"[Train Epoch {epoch}] Avg Loss: {avg_loss}, Accuracy: {total_correct}/{total_count} ({correct_perc:.2f}%)\")\n",
    "\n",
    "  def test_epoch(self, epoch):\n",
    "    self.model.eval()\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "      iterator = tqdm(self.test_loader)\n",
    "      for (i, (x, y)) in enumerate(iterator):\n",
    "        y_pred = self.model(x).to(\"cpu\")\n",
    "        loss = self.loss(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        (num_correct, batch_size) = self.accuracy(y_pred, y)\n",
    "        total_count += batch_size\n",
    "        total_correct += num_correct\n",
    "        correct_perc = 100. * total_correct / total_count\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        iterator.set_description(f\"[Test Epoch {epoch}] Avg Loss: {avg_loss}, Accuracy: {total_correct}/{total_count} ({correct_perc:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root =  \"./data\"\n",
    "\n",
    "(train_loader, test_loader) = clutrr_loader(data_root, train, test, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roberta-base' is the correct path to a directory containing all relevant files for a RobertaTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_mlp_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_last_hidden_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, train_loader, test_loader, device, learning_rate, **args)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_loader, test_loader, device, learning_rate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m----> 4\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mCLUTRRModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m train_loader\n",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36mCLUTRRModel.__init__\u001b[0;34m(self, device, num_mlp_layers, debug, use_last_hidden_state)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_last_hidden_state \u001b[38;5;241m=\u001b[39m use_last_hidden_state\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Roberta as embedding extraction model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroberta-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta_model \u001b[38;5;241m=\u001b[39m RobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_Project/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2070\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2065\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2066\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2067\u001b[0m     )\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2073\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2074\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2075\u001b[0m     )\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2078\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roberta-base' is the correct path to a directory containing all relevant files for a RobertaTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(train_loader, test_loader, \"cpu\", 0.00001, num_mlp_layers=1, debug=\"store_true\", use_last_hidden_state= \"store_true\")\n",
    "trainer.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--n-epochs N_EPOCHS]\n",
      "                             [--batch-size BATCH_SIZE] [--seed SEED]\n",
      "                             [--learning-rate LEARNING_RATE]\n",
      "                             [--num-mlp-layers NUM_MLP_LAYERS] [--debug]\n",
      "                             [--use-last-hidden-state] [--cuda] [--gpu GPU]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/hrishikesh/Library/Jupyter/runtime/kernel-v2-57751fneXzxYSodYf.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "relation_id_map = {\n",
    "  'daughter': 0,\n",
    "  'sister': 1,\n",
    "  'son': 2,\n",
    "  'aunt': 3,\n",
    "  'father': 4,\n",
    "  'husband': 5,\n",
    "  'granddaughter': 6,\n",
    "  'brother': 7,\n",
    "  'nephew': 8,\n",
    "  'mother': 9,\n",
    "  'uncle': 10,\n",
    "  'grandfather': 11,\n",
    "  'wife': 12,\n",
    "  'grandmother': 13,\n",
    "  'niece': 14,\n",
    "  'grandson': 15,\n",
    "  'son-in-law': 16,\n",
    "  'father-in-law': 17,\n",
    "  'daughter-in-law': 18,\n",
    "  'mother-in-law': 19,\n",
    "}\n",
    "\n",
    "class CLUTRRDataset:\n",
    "  def __init__(self, root, dataset, split):\n",
    "    self.dataset_dir = os.path.join(root, f\"{dataset}/\")\n",
    "    self.file_names = [os.path.join(self.dataset_dir, d) for d in os.listdir(self.dataset_dir) if f\"_{split}.csv\" in d]\n",
    "    self.data = [row for f in self.file_names for row in list(csv.reader(open(f)))[1:]]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    # Context is a list of sentences\n",
    "    context = [s.strip().lower() for s in self.data[i][2].split(\".\") if s.strip() != \"\"]\n",
    "\n",
    "    # Query is of type (sub, obj)\n",
    "    query_sub_obj = eval(self.data[i][3])\n",
    "    query = (query_sub_obj[0].lower(), query_sub_obj[1].lower())\n",
    "\n",
    "    # Answer is one of 20 classes such as daughter, mother, ...\n",
    "    answer = self.data[i][5]\n",
    "    return ((context, query), answer)\n",
    "\n",
    "  @staticmethod\n",
    "  def collate_fn(batch):\n",
    "    queries = [query for ((_, query), _) in batch]\n",
    "    contexts = [fact for ((context, _), _) in batch for fact in context]\n",
    "    context_lens = [len(context) for ((context, _), _) in batch]\n",
    "    context_splits = [(sum(context_lens[:i]), sum(context_lens[:i + 1])) for i in range(len(context_lens))]\n",
    "    answers = torch.stack([torch.tensor(relation_id_map[answer]) for (_, answer) in batch])\n",
    "    return ((contexts, queries, context_splits), answers)\n",
    "\n",
    "\n",
    "def clutrr_loader(root, dataset, batch_size):\n",
    "  train_dataset = CLUTRRDataset(root, dataset, \"train\")\n",
    "  train_loader = DataLoader(train_dataset, batch_size, collate_fn=CLUTRRDataset.collate_fn, shuffle=True)\n",
    "  test_dataset = CLUTRRDataset(root, dataset, \"test\")\n",
    "  test_loader = DataLoader(test_dataset, batch_size, collate_fn=CLUTRRDataset.collate_fn, shuffle=True)\n",
    "  return (train_loader, test_loader)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, in_dim: int, embed_dim: int, out_dim: int, num_layers: int = 0, softmax = False, normalize = False, sigmoid = False):\n",
    "    super(MLP, self).__init__()\n",
    "    layers = []\n",
    "    layers += [nn.Linear(in_dim, embed_dim), nn.ReLU()]\n",
    "    for _ in range(num_layers):\n",
    "      layers += [nn.Linear(embed_dim, embed_dim), nn.ReLU()]\n",
    "    layers += [nn.Linear(embed_dim, out_dim)]\n",
    "    self.model = nn.Sequential(*layers)\n",
    "    self.softmax = softmax\n",
    "    self.normalize = normalize\n",
    "    self.sigmoid = sigmoid\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    if self.softmax: x = nn.functional.softmax(x, dim=1)\n",
    "    if self.normalize: x = nn.functional.normalize(x)\n",
    "    if self.sigmoid: x = torch.sigmoid(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class CLUTRRModel(nn.Module):\n",
    "  def __init__(self, device=\"cpu\", num_mlp_layers=0, debug=False, use_last_hidden_state=False):\n",
    "    super(CLUTRRModel, self).__init__()\n",
    "\n",
    "    # Options\n",
    "    self.device = device\n",
    "    self.debug = debug\n",
    "    self.use_last_hidden_state = use_last_hidden_state\n",
    "\n",
    "    # Roberta as embedding extraction model\n",
    "    self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", local_files_only=True, add_prefix_space=True)\n",
    "    self.roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "    self.embed_dim = self.roberta_model.config.hidden_size\n",
    "\n",
    "    # Entity embedding\n",
    "    self.relation_extraction = MLP(self.embed_dim, self.embed_dim, len(relation_id_map), num_layers=num_mlp_layers, sigmoid=True)\n",
    "\n",
    "    # Scallop reasoning context\n",
    "    self.scallop_ctx = scallopy.ScallopContext(\"difftopbottomkclauses\", k=3)\n",
    "    self.scallop_ctx.import_file(os.path.abspath(os.path.join(os.path.abspath(__file__), \"../scl/clutrr.scl\")))\n",
    "    self.scallop_ctx.set_non_probabilistic([\"question\"])\n",
    "    if self.debug:\n",
    "      self.reason = self.scallop_ctx.forward_function(\"answer\", list(range(len(relation_id_map))), dispatch=\"single\", debug_provenance=True)\n",
    "    else:\n",
    "      self.reason = self.scallop_ctx.forward_function(\"answer\", list(range(len(relation_id_map))))\n",
    "\n",
    "  def forward(self, x):\n",
    "    (contexts, queries, context_splits) = x\n",
    "    batch_size = len(context_splits)\n",
    "\n",
    "    if self.debug:\n",
    "      print(contexts)\n",
    "      print(queries)\n",
    "\n",
    "    # Preprocess sentences\n",
    "    relation_splits = []\n",
    "    relation_sentences = []\n",
    "    relation_name_pairs = []\n",
    "    for (_, (start, end)) in enumerate(context_splits):\n",
    "      curr_relation_sentences = []\n",
    "      curr_name_pairs = []\n",
    "      skip_next = False\n",
    "      skip_until = 0\n",
    "      for (j, sentence) in zip(range(start, end), contexts[start:end]):\n",
    "        # It is possible to skip a sentence because the previous one includes the current one.\n",
    "        if skip_next:\n",
    "          if j >= skip_until:\n",
    "            skip_next = False\n",
    "          continue\n",
    "\n",
    "        # Get all the names of the current sentence\n",
    "        names = re.findall(\"\\\\[(\\w+)\\\\]\", sentence)\n",
    "\n",
    "        # Check if we need to include the next sentence(s) as well\n",
    "        num_sentences_limit = 4\n",
    "        num_sentences = 1\n",
    "        union_sentence = f\"{sentence}\"\n",
    "        for k in range(j + 1, end):\n",
    "          next_sentence = contexts[k]\n",
    "          next_sentence_names = re.findall(\"\\\\[(\\w+)\\\\]\", next_sentence)\n",
    "          if (len(names) == 1 or len(next_sentence_names) == 1) and num_sentences < num_sentences_limit:\n",
    "            if len(next_sentence_names) > 0:\n",
    "              num_sentences += 1\n",
    "              union_sentence += f\". {next_sentence}\"\n",
    "              names += next_sentence_names\n",
    "            skip_next = True\n",
    "            if len(next_sentence_names) == 1:\n",
    "              skip_until = k - 1\n",
    "            else:\n",
    "              skip_until = k\n",
    "          else:\n",
    "            break\n",
    "\n",
    "        # Deduplicate the names\n",
    "        names = list(dict.fromkeys(names))\n",
    "\n",
    "        # Debug number of sentences\n",
    "        if self.debug and num_sentences > 1:\n",
    "          print(f\"number of sentences: {num_sentences}, number of names: {len(names)}; {names}\")\n",
    "          print(\"Sentence:\", union_sentence)\n",
    "\n",
    "        # Clean up the sentence and add it to the batch\n",
    "        clean_sentence = union_sentence.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        curr_relation_sentences += [f\"{clean_sentence}. the relation between {names[k]} and {names[l]} is?\" for k in range(len(names)) for l in range(len(names)) if k != l]\n",
    "        curr_name_pairs += [(k, l) for k in names for l in names if k != l]\n",
    "\n",
    "      # Construct the current datatpoint\n",
    "      curr_split = (0, len(curr_relation_sentences)) if len(relation_sentences) == 0 else (relation_splits[-1][1], relation_splits[-1][1] + len(curr_relation_sentences))\n",
    "      relation_sentences += curr_relation_sentences\n",
    "      relation_name_pairs += curr_name_pairs\n",
    "      relation_splits.append(curr_split)\n",
    "\n",
    "    # Embed all these sentences\n",
    "    sentence_tokens = self.tokenizer(relation_sentences, padding=True, return_tensors=\"pt\")\n",
    "    sentence_input_ids = sentence_tokens[\"input_ids\"].to(self.device)\n",
    "    sentence_attention_mask = sentence_tokens[\"attention_mask\"].to(self.device)\n",
    "    encoded_sentence = self.roberta_model(sentence_input_ids, sentence_attention_mask)\n",
    "    if self.use_last_hidden_state:\n",
    "      sentence_embeddings = encoded_sentence.last_hidden_state[:, 0, :]\n",
    "    else:\n",
    "      sentence_embeddings = encoded_sentence.pooler_output\n",
    "    relations = self.relation_extraction(sentence_embeddings)\n",
    "\n",
    "    # Construct facts\n",
    "    question_facts = [[] for _ in range(batch_size)]\n",
    "    context_facts = [[] for _ in range(batch_size)]\n",
    "    for (i, (start, end)) in enumerate(relation_splits):\n",
    "      question_facts[i] = [queries[i]]\n",
    "      context_facts[i] = [(relations[j, k], (k, relation_name_pairs[j][0], relation_name_pairs[j][1])) for j in range(start, end) for k in range(len(relation_id_map))]\n",
    "\n",
    "    # Run scallop\n",
    "    result = self.reason(question=question_facts, context=context_facts)\n",
    "\n",
    "    # Softmax the result\n",
    "    result = nn.functional.softmax(result, dim=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, train_loader, test_loader, device, learning_rate, **args):\n",
    "    self.device = device\n",
    "    self.model = CLUTRRModel(device=device, **args).to(device)\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    self.train_loader = train_loader\n",
    "    self.test_loader = test_loader\n",
    "\n",
    "  def loss(self, y_pred, y):\n",
    "    (_, dim) = y_pred.shape\n",
    "    gt = torch.stack([torch.tensor([1.0 if i == t else 0.0 for i in range(dim)]) for t in y])\n",
    "    return nn.functional.binary_cross_entropy(y_pred, gt)\n",
    "\n",
    "  def accuracy(self, y_pred, y):\n",
    "    batch_size = len(y)\n",
    "    pred = torch.argmax(y_pred, dim=1)\n",
    "    num_correct = len([() for i, j in zip(pred, y) if i == j])\n",
    "    return (num_correct, batch_size)\n",
    "\n",
    "  def train(self, num_epochs):\n",
    "    for i in range(num_epochs):\n",
    "      self.train_epoch(i)\n",
    "      self.test_epoch(i)\n",
    "\n",
    "  def train_epoch(self, epoch):\n",
    "    self.model.train()\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    iterator = tqdm(self.train_loader)\n",
    "    for (i, (x, y)) in enumerate(iterator):\n",
    "      self.optimizer.zero_grad()\n",
    "      y_pred = self.model(x).to(\"cpu\")\n",
    "      loss = self.loss(y_pred, y)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "      (num_correct, batch_size) = self.accuracy(y_pred, y)\n",
    "      total_count += batch_size\n",
    "      total_correct += num_correct\n",
    "      correct_perc = 100. * total_correct / total_count\n",
    "      avg_loss = total_loss / (i + 1)\n",
    "\n",
    "      iterator.set_description(f\"[Train Epoch {epoch}] Avg Loss: {avg_loss}, Accuracy: {total_correct}/{total_count} ({correct_perc:.2f}%)\")\n",
    "\n",
    "  def test_epoch(self, epoch):\n",
    "    self.model.eval()\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "      iterator = tqdm(self.test_loader)\n",
    "      for (i, (x, y)) in enumerate(iterator):\n",
    "        y_pred = self.model(x).to(\"cpu\")\n",
    "        loss = self.loss(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        (num_correct, batch_size) = self.accuracy(y_pred, y)\n",
    "        total_count += batch_size\n",
    "        total_correct += num_correct\n",
    "        correct_perc = 100. * total_correct / total_count\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        iterator.set_description(f\"[Test Epoch {epoch}] Avg Loss: {avg_loss}, Accuracy: {total_correct}/{total_count} ({correct_perc:.2f}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  parser = ArgumentParser()\n",
    "  \n",
    "  parser.add_argument(\"--dataset\", type=str, default=\"data_089907f8\")\n",
    "  parser.add_argument(\"--n-epochs\", type=int, default=100)\n",
    "  parser.add_argument(\"--batch-size\", type=int, default=16)\n",
    "  parser.add_argument(\"--seed\", type=int, default=1234)\n",
    "  parser.add_argument(\"--learning-rate\", type=float, default=0.00001)\n",
    "  parser.add_argument(\"--num-mlp-layers\", type=int, default=1)\n",
    "  parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "  parser.add_argument(\"--use-last-hidden-state\", action=\"store_true\")\n",
    "  parser.add_argument(\"--cuda\", action=\"store_true\")\n",
    "  parser.add_argument(\"--gpu\", type=int, default=0)\n",
    "  args = parser.parse_args()\n",
    "\n",
    "\n",
    "  # Parameters\n",
    "  torch.manual_seed(args.seed)\n",
    "  random.seed(args.seed)\n",
    "  if args.cuda:\n",
    "    if torch.cuda.is_available(): device = torch.device(f\"cuda:{args.gpu}\")\n",
    "    else: raise Exception(\"No cuda available\")\n",
    "  else: device = torch.device(\"cpu\")\n",
    "\n",
    "  # Loading dataset\n",
    "  data_root = os.path.abspath(os.path.join(os.path.abspath(__file__), \"../../data\"))\n",
    "  (train_loader, test_loader) = clutrr_loader(data_root, args.dataset, args.batch_size)\n",
    "\n",
    "  # Train\n",
    "  # trainer = Trainer(train_loader, test_loader, device, args.learning_rate, num_mlp_layers=args.num_mlp_layers, debug=args.debug, use_last_hidden_state=args.use_last_hidden_state)\n",
    "  # trainer.train(args.n_epochs)\n",
    "  trainer = Trainer(train_loader, test_loader, device, 0.00001, 1, \"store_true\", \"store_true\")\n",
    "  trainer.train(args.n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
