{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scallopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaModel, RobertaTokenizer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscallopy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scallopy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "import scallopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: scallopy-0.1.4-cp39-cp39-macosx_10_7_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install scallopy-0.1.4-cp39-cp39-macosx_10_7_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: datasets in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (8.1.2)\n",
      "Requirement already satisfied: torch in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: transformers in /Users/hrishikesh/miniconda3/envs/NLP_Project/lib/python3.11/site-packages (4.39.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scallopy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scallopy\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install pandas datasets ipywidgets torch transformers scallopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['gen_train234_test2to10', 'gen_train23_test2to10', 'rob_train_clean_23_test_all_23', 'rob_train_disc_23_test_all_23', 'rob_train_irr_23_test_all_23', 'rob_train_sup_23_test_all_23']\nExample of usage:\n\t`load_dataset('v1', 'gen_train234_test2to10')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCLUTRR/v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_Project/lib/python3.11/site-packages/datasets/load.py:2556\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2552\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2556\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2573\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_Project/lib/python3.11/site-packages/datasets/load.py:2265\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2263\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 2265\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2276\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_Project/lib/python3.11/site-packages/datasets/builder.py:371\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP_Project/lib/python3.11/site-packages/datasets/builder.py:577\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_kwargs:\n\u001b[1;32m    576\u001b[0m         example_of_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_dataset(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig name is missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease pick one among the available configs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample of usage:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_of_usage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['gen_train234_test2to10', 'gen_train23_test2to10', 'rob_train_clean_23_test_all_23', 'rob_train_disc_23_test_all_23', 'rob_train_irr_23_test_all_23', 'rob_train_sup_23_test_all_23']\nExample of usage:\n\t`load_dataset('v1', 'gen_train234_test2to10')`"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "relation_id_map = {\n",
    "  'daughter': 0,\n",
    "  'sister': 1,\n",
    "  'son': 2,\n",
    "  'aunt': 3,\n",
    "  'father': 4,\n",
    "  'husband': 5,\n",
    "  'granddaughter': 6,\n",
    "  'brother': 7,\n",
    "  'nephew': 8,\n",
    "  'mother': 9,\n",
    "  'uncle': 10,\n",
    "  'grandfather': 11,\n",
    "  'wife': 12,\n",
    "  'grandmother': 13,\n",
    "  'niece': 14,\n",
    "  'grandson': 15,\n",
    "  'son-in-law': 16,\n",
    "  'father-in-law': 17,\n",
    "  'daughter-in-law': 18,\n",
    "  'mother-in-law': 19,\n",
    "}\n",
    "\n",
    "class CLUTRRDataset:\n",
    "  def __init__(self, root, dataset, split):\n",
    "    self.dataset_dir = os.path.join(root, f\"{dataset}/\")\n",
    "    self.file_names = [os.path.join(self.dataset_dir, d) for d in os.listdir(self.dataset_dir) if f\"_{split}.csv\" in d]\n",
    "    self.data = [row for f in self.file_names for row in list(csv.reader(open(f)))[1:]]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    # Context is a list of sentences\n",
    "    context = [s.strip().lower() for s in self.data[i][2].split(\".\") if s.strip() != \"\"]\n",
    "\n",
    "    # Query is of type (sub, obj)\n",
    "    query_sub_obj = eval(self.data[i][3])\n",
    "    query = (query_sub_obj[0].lower(), query_sub_obj[1].lower())\n",
    "\n",
    "    # Answer is one of 20 classes such as daughter, mother, ...\n",
    "    answer = self.data[i][5]\n",
    "    return ((context, query), answer)\n",
    "\n",
    "  @staticmethod\n",
    "  def collate_fn(batch):\n",
    "    queries = [query for ((_, query), _) in batch]\n",
    "    contexts = [fact for ((context, _), _) in batch for fact in context]\n",
    "    context_lens = [len(context) for ((context, _), _) in batch]\n",
    "    context_splits = [(sum(context_lens[:i]), sum(context_lens[:i + 1])) for i in range(len(context_lens))]\n",
    "    answers = torch.stack([torch.tensor(relation_id_map[answer]) for (_, answer) in batch])\n",
    "    return ((contexts, queries, context_splits), answers)\n",
    "\n",
    "\n",
    "def clutrr_loader(root, dataset, batch_size):\n",
    "  train_dataset = CLUTRRDataset(root, dataset, \"train\")\n",
    "  train_loader = DataLoader(train_dataset, batch_size, collate_fn=CLUTRRDataset.collate_fn, shuffle=True)\n",
    "  test_dataset = CLUTRRDataset(root, dataset, \"test\")\n",
    "  test_loader = DataLoader(test_dataset, batch_size, collate_fn=CLUTRRDataset.collate_fn, shuffle=True)\n",
    "  return (train_loader, test_loader)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, in_dim: int, embed_dim: int, out_dim: int, num_layers: int = 0, softmax = False, normalize = False, sigmoid = False):\n",
    "    super(MLP, self).__init__()\n",
    "    layers = []\n",
    "    layers += [nn.Linear(in_dim, embed_dim), nn.ReLU()]\n",
    "    for _ in range(num_layers):\n",
    "      layers += [nn.Linear(embed_dim, embed_dim), nn.ReLU()]\n",
    "    layers += [nn.Linear(embed_dim, out_dim)]\n",
    "    self.model = nn.Sequential(*layers)\n",
    "    self.softmax = softmax\n",
    "    self.normalize = normalize\n",
    "    self.sigmoid = sigmoid\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    if self.softmax: x = nn.functional.softmax(x, dim=1)\n",
    "    if self.normalize: x = nn.functional.normalize(x)\n",
    "    if self.sigmoid: x = torch.sigmoid(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class CLUTRRModel(nn.Module):\n",
    "  def __init__(self, device=\"cpu\", num_mlp_layers=0, debug=False, use_last_hidden_state=False):\n",
    "    super(CLUTRRModel, self).__init__()\n",
    "\n",
    "    # Options\n",
    "    self.device = device\n",
    "    self.debug = debug\n",
    "    self.use_last_hidden_state = use_last_hidden_state\n",
    "\n",
    "    # Roberta as embedding extraction model\n",
    "    self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", local_files_only=True, add_prefix_space=True)\n",
    "    self.roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "    self.embed_dim = self.roberta_model.config.hidden_size\n",
    "\n",
    "    # Entity embedding\n",
    "    self.relation_extraction = MLP(self.embed_dim, self.embed_dim, len(relation_id_map), num_layers=num_mlp_layers, sigmoid=True)\n",
    "\n",
    "    # Scallop reasoning context\n",
    "    self.scallop_ctx = scallopy.ScallopContext(\"difftopbottomkclauses\", k=3)\n",
    "    self.scallop_ctx.import_file(os.path.abspath(os.path.join(os.path.abspath(__file__), \"../scl/clutrr.scl\")))\n",
    "    self.scallop_ctx.set_non_probabilistic([\"question\"])\n",
    "    if self.debug:\n",
    "      self.reason = self.scallop_ctx.forward_function(\"answer\", list(range(len(relation_id_map))), dispatch=\"single\", debug_provenance=True)\n",
    "    else:\n",
    "      self.reason = self.scallop_ctx.forward_function(\"answer\", list(range(len(relation_id_map))))\n",
    "\n",
    "  def forward(self, x):\n",
    "    (contexts, queries, context_splits) = x\n",
    "    batch_size = len(context_splits)\n",
    "\n",
    "    if self.debug:\n",
    "      print(contexts)\n",
    "      print(queries)\n",
    "\n",
    "    # Preprocess sentences\n",
    "    relation_splits = []\n",
    "    relation_sentences = []\n",
    "    relation_name_pairs = []\n",
    "    for (_, (start, end)) in enumerate(context_splits):\n",
    "      curr_relation_sentences = []\n",
    "      curr_name_pairs = []\n",
    "      skip_next = False\n",
    "      skip_until = 0\n",
    "      for (j, sentence) in zip(range(start, end), contexts[start:end]):\n",
    "        # It is possible to skip a sentence because the previous one includes the current one.\n",
    "        if skip_next:\n",
    "          if j >= skip_until:\n",
    "            skip_next = False\n",
    "          continue\n",
    "\n",
    "        # Get all the names of the current sentence\n",
    "        names = re.findall(\"\\\\[(\\w+)\\\\]\", sentence)\n",
    "\n",
    "        # Check if we need to include the next sentence(s) as well\n",
    "        num_sentences_limit = 4\n",
    "        num_sentences = 1\n",
    "        union_sentence = f\"{sentence}\"\n",
    "        for k in range(j + 1, end):\n",
    "          next_sentence = contexts[k]\n",
    "          next_sentence_names = re.findall(\"\\\\[(\\w+)\\\\]\", next_sentence)\n",
    "          if (len(names) == 1 or len(next_sentence_names) == 1) and num_sentences < num_sentences_limit:\n",
    "            if len(next_sentence_names) > 0:\n",
    "              num_sentences += 1\n",
    "              union_sentence += f\". {next_sentence}\"\n",
    "              names += next_sentence_names\n",
    "            skip_next = True\n",
    "            if len(next_sentence_names) == 1:\n",
    "              skip_until = k - 1\n",
    "            else:\n",
    "              skip_until = k\n",
    "          else:\n",
    "            break\n",
    "\n",
    "        # Deduplicate the names\n",
    "        names = list(dict.fromkeys(names))\n",
    "\n",
    "        # Debug number of sentences\n",
    "        if self.debug and num_sentences > 1:\n",
    "          print(f\"number of sentences: {num_sentences}, number of names: {len(names)}; {names}\")\n",
    "          print(\"Sentence:\", union_sentence)\n",
    "\n",
    "        # Clean up the sentence and add it to the batch\n",
    "        clean_sentence = union_sentence.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        curr_relation_sentences += [f\"{clean_sentence}. the relation between {names[k]} and {names[l]} is?\" for k in range(len(names)) for l in range(len(names)) if k != l]\n",
    "        curr_name_pairs += [(k, l) for k in names for l in names if k != l]\n",
    "\n",
    "      # Construct the current datatpoint\n",
    "      curr_split = (0, len(curr_relation_sentences)) if len(relation_sentences) == 0 else (relation_splits[-1][1], relation_splits[-1][1] + len(curr_relation_sentences))\n",
    "      relation_sentences += curr_relation_sentences\n",
    "      relation_name_pairs += curr_name_pairs\n",
    "      relation_splits.append(curr_split)\n",
    "\n",
    "    # Embed all these sentences\n",
    "    sentence_tokens = self.tokenizer(relation_sentences, padding=True, return_tensors=\"pt\")\n",
    "    sentence_input_ids = sentence_tokens[\"input_ids\"].to(self.device)\n",
    "    sentence_attention_mask = sentence_tokens[\"attention_mask\"].to(self.device)\n",
    "    encoded_sentence = self.roberta_model(sentence_input_ids, sentence_attention_mask)\n",
    "    if self.use_last_hidden_state:\n",
    "      sentence_embeddings = encoded_sentence.last_hidden_state[:, 0, :]\n",
    "    else:\n",
    "      sentence_embeddings = encoded_sentence.pooler_output\n",
    "    relations = self.relation_extraction(sentence_embeddings)\n",
    "\n",
    "    # Construct facts\n",
    "    question_facts = [[] for _ in range(batch_size)]\n",
    "    context_facts = [[] for _ in range(batch_size)]\n",
    "    for (i, (start, end)) in enumerate(relation_splits):\n",
    "      question_facts[i] = [queries[i]]\n",
    "      context_facts[i] = [(relations[j, k], (k, relation_name_pairs[j][0], relation_name_pairs[j][1])) for j in range(start, end) for k in range(len(relation_id_map))]\n",
    "\n",
    "    # Run scallop\n",
    "    result = self.reason(question=question_facts, context=context_facts)\n",
    "\n",
    "    # Softmax the result\n",
    "    result = nn.functional.softmax(result, dim=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, train_loader, test_loader, device, learning_rate, **args):\n",
    "    self.device = device\n",
    "    self.model = CLUTRRModel(device=device, **args).to(device)\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    self.train_loader = train_loader\n",
    "    self.test_loader = test_loader\n",
    "\n",
    "  def loss(self, y_pred, y):\n",
    "    (_, dim) = y_pred.shape\n",
    "    gt = torch.stack([torch.tensor([1.0 if i == t else 0.0 for i in range(dim)]) for t in y])\n",
    "    return nn.functional.binary_cross_entropy(y_pred, gt)\n",
    "\n",
    "  def accuracy(self, y_pred, y):\n",
    "    batch_size = len(y)\n",
    "    pred = torch.argmax(y_pred, dim=1)\n",
    "    num_correct = len([() for i, j in zip(pred, y) if i == j])\n",
    "    return (num_correct, batch_size)\n",
    "\n",
    "  def train(self, num_epochs):\n",
    "    for i in range(num_epochs):\n",
    "      self.train_epoch(i)\n",
    "      self.test_epoch(i)\n",
    "\n",
    "  def train_epoch(self, epoch):\n",
    "    self.model.train()\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    iterator = tqdm(self.train_loader)\n",
    "    for (i, (x, y)) in enumerate(iterator):\n",
    "      self.optimizer.zero_grad()\n",
    "      y_pred = self.model(x).to(\"cpu\")\n",
    "      loss = self.loss(y_pred, y)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "      (num_correct, batch_size) = self.accuracy(y_pred, y)\n",
    "      total_count += batch_size\n",
    "      total_correct += num_correct\n",
    "      correct_perc = 100. * total_correct / total_count\n",
    "      avg_loss = total_loss / (i + 1)\n",
    "\n",
    "      iterator.set_description(f\"[Train Epoch {epoch}] Avg Loss: {avg_loss}, Accuracy: {total_correct}/{total_count} ({correct_perc:.2f}%)\")\n",
    "\n",
    "  def test_epoch(self, epoch):\n",
    "    self.model.eval()\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "      iterator = tqdm(self.test_loader)\n",
    "      for (i, (x, y)) in enumerate(iterator):\n",
    "        y_pred = self.model(x).to(\"cpu\")\n",
    "        loss = self.loss(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        (num_correct, batch_size) = self.accuracy(y_pred, y)\n",
    "        total_count += batch_size\n",
    "        total_correct += num_correct\n",
    "        correct_perc = 100. * total_correct / total_count\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        iterator.set_description(f\"[Test Epoch {epoch}] Avg Loss: {avg_loss}, Accuracy: {total_correct}/{total_count} ({correct_perc:.2f}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = ArgumentParser()\n",
    "  parser.add_argument(\"--dataset\", type=str, default=\"data_089907f8\")\n",
    "  parser.add_argument(\"--n-epochs\", type=int, default=100)\n",
    "  parser.add_argument(\"--batch-size\", type=int, default=16)\n",
    "  parser.add_argument(\"--seed\", type=int, default=1234)\n",
    "  parser.add_argument(\"--learning-rate\", type=float, default=0.00001)\n",
    "  parser.add_argument(\"--num-mlp-layers\", type=int, default=1)\n",
    "  parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "  parser.add_argument(\"--use-last-hidden-state\", action=\"store_true\")\n",
    "  parser.add_argument(\"--cuda\", action=\"store_true\")\n",
    "  parser.add_argument(\"--gpu\", type=int, default=0)\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  # Parameters\n",
    "  torch.manual_seed(args.seed)\n",
    "  random.seed(args.seed)\n",
    "  if args.cuda:\n",
    "    if torch.cuda.is_available(): device = torch.device(f\"cuda:{args.gpu}\")\n",
    "    else: raise Exception(\"No cuda available\")\n",
    "  else: device = torch.device(\"cpu\")\n",
    "\n",
    "  # Loading dataset\n",
    "  data_root = os.path.abspath(os.path.join(os.path.abspath(__file__), \"../../data\"))\n",
    "  (train_loader, test_loader) = clutrr_loader(data_root, args.dataset, args.batch_size)\n",
    "\n",
    "  # Train\n",
    "  trainer = Trainer(train_loader, test_loader, device, args.learning_rate, num_mlp_layers=args.num_mlp_layers, debug=args.debug, use_last_hidden_state=args.use_last_hidden_state)\n",
    "  trainer.train(args.n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
